{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## **Interpretability via Attentional and Memory-based Interfaces Using TensorFlow**\n",
    "#### A closer look at the reasoning inside your deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### **Table of Contents**\n",
    "1. [Applying Selective Attention to Deep Learning\n",
    "](#1)\n",
    "2. [Overview of Attentional Interfaces](#2)\n",
    "3. [Basic Sentiment Analysis and Overview Architecture](#3)\n",
    "4. [Setup](#4)\n",
    "5. [Preprocessing Components](#5)\n",
    "6. [Sample the Data](#6)\n",
    "7. [Model](#7)\n",
    "7. [Training](#8)\n",
    "9. [Results](#9)\n",
    "10. [Attention for a Sample](#10)\n",
    "11. [Attentional History](#11)\n",
    "12. [Attentional Interface Variants](#12)\n",
    "13. [Caveats](#13)\n",
    "13. [Interpretability and why itâ€™s important](#14)\n",
    "14. [References](#15)\n",
    "15. [Author Bio](#16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Establish basedir (useful if running as python package)\n",
    "import os\n",
    "basedir = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Hide all warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='5'></a>\n",
    "### **V. Preprocessing Components**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In this section, we will preprocess our raw input data. The main components are the Vocab class, which we initialize using our vocab.txt file. This file contains all of the tokens (words) from our raw input, sorted by descending frequency. The next helper function we need is ids_to_tokens(), which will convert a list of ids into tokens we can understand. We will use this for reading our input and associating the word with its respective attention score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     11,
     15,
     19,
     23
    ],
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess the reviews.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from unidecode import (\n",
    "    unidecode,\n",
    ")\n",
    "\n",
    "from random import (\n",
    "    shuffle,\n",
    ")\n",
    "\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")\n",
    "\n",
    "from collections import (\n",
    "    Counter,\n",
    ")\n",
    "\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     63
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    \"\"\"\n",
    "    Class for processing tokens to ids and vice versa.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_file, max_vocab_size=200000, verbose=True):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        self._token_to_id = {}\n",
    "        self._id_to_token = {}\n",
    "        self._size = -1\n",
    "\n",
    "        with open(vocab_file, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = line.split()\n",
    "\n",
    "                # White space in vocab file (' ': <count>)\n",
    "                if len(tokens) == 1:\n",
    "                    count = tokens[0]\n",
    "                    idx = line.index(count)\n",
    "                    t = line[:idx-1]\n",
    "                    tokens = (t, count)\n",
    "\n",
    "                if len(tokens) != 2:\n",
    "                    continue\n",
    "\n",
    "                if tokens[0] in self._token_to_id:\n",
    "                    continue\n",
    "\n",
    "                self._size += 1\n",
    "                if self._size > max_vocab_size:\n",
    "                    print ('Too many tokens! >%i/n' % max_vocab_size)\n",
    "                    break\n",
    "\n",
    "                self._token_to_id[tokens[0]] = self._size\n",
    "                self._id_to_token[self._size] = tokens[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return vocabulary size.\n",
    "        \"\"\"\n",
    "        return self._size+1\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        Return the corresponding id for a token.\n",
    "        \"\"\"\n",
    "        if token not in self._token_to_id:\n",
    "            if self.verbose:\n",
    "                print (\"ID not found for %s\" % token)\n",
    "            return self._token_to_id[UNKNOWN_TOKEN]\n",
    "        return self._token_to_id[token]\n",
    "\n",
    "    def id_to_token(self, _id):\n",
    "        \"\"\"\n",
    "        Returnn the correspoding token for an id.\n",
    "        \"\"\"\n",
    "        if _id not in self._id_to_token:\n",
    "            if self.verbose:\n",
    "                print (\"Token not found for ID: %i\" % _id)\n",
    "            return UNKNOWN_TOKEN\n",
    "        return self._id_to_token[_id]\n",
    "\n",
    "def ids_to_tokens(ids_list, vocab):\n",
    "    \"\"\"\n",
    "    Convert a list of ids to tokens.\n",
    "    Args:\n",
    "        ids_list: list of ids to convert to tokens.\n",
    "        vocab: Vocab class object.\n",
    "    Returns:\n",
    "        answer: list of tokens that corresponds to ids_list.\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    for _id in ids_list:\n",
    "        token = vocab.id_to_token(_id)\n",
    "        if token == PAD_TOKEN:\n",
    "            continue\n",
    "        answer.append(token)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='6'></a>\n",
    "### **VI. Sample the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Now, we will see what our inputs will look like. The processed_review represents our reviews with ids. The `review_seq_len` tells us how long the review is. Unless we use dynamic computation graphs, we need to feed in fixed sized inputs into our TensorFlow models per batch. This means that we will have some padding (with `PAD`s) and we do not want these to influence our model. In this implementation, the PADs do not prove to be too problematic, since inference will depend on the entire summarized context (so no loss masking is needed). And we also want to keep the PAD tokens, even when determining the attention scores, to show how the model learns not to focus on the PADs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"  \n",
    "        self.data_dir=\"data/processed_reviews/train.p\"           # location of reviews data (train|validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def sample_data(data_path):\n",
    "    \"\"\"\n",
    "    Sample format of the processed\n",
    "    data from data.py\n",
    "    Args:\n",
    "        data_path: path for train.p|valid.p\n",
    "    \"\"\"\n",
    "    with open(data_path, 'rb') as f:\n",
    "        entries = pickle.load(f)\n",
    "\n",
    "    # Choose a random sample\n",
    "    rand_index = random.randint(0, len(entries))\n",
    "\n",
    "    # Prepare vocab\n",
    "    vocab_file = os.path.join(basedir, 'data/processed_reviews/vocab.txt')\n",
    "    vocab = Vocab(vocab_file, verbose=False)\n",
    "\n",
    "    # Sample\n",
    "    (processed_review,\n",
    "     review_seq_len,\n",
    "     label) = entries[rand_index]\n",
    "\n",
    "    print (\"==> Number of entries:\", len(entries))\n",
    "    print (\"==> Random index:\", rand_index)\n",
    "    print (\"==> Processed Review:\", processed_review)\n",
    "    print (\"==> Review Len:\", review_seq_len)\n",
    "    print (\"==> Label:\", label)\n",
    "    print (\"==> See if processed review makes sense:\",\n",
    "        ids_to_tokens(\n",
    "            processed_review,\n",
    "            vocab=vocab,\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "sample_data(FLAGS.data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='7'></a>\n",
    "### **VII. Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We will start by talking about operation functions. `_xavier_weight_init()` is a little function we made to properly initialize our weights, depending on the nonlinearity that will be applied to them. The initialization is such that we will receive outputs with unit variance prior to sending to the activation function. \n",
    "\n",
    "This is an optimization technique we use so that we do not have large values when applying the nonlinearity, as that will lead to saturation at the extremes and lead to gradient issues. We also have a helper function for layer normalization, `ln()`, which is another optimization technique that will normalize our inputs into the GRU (Gated Recurrent Unit) before applying the activation function. This will allow us to control gradient issues and even allow us to use larger learning rates. The layer normalization is applied in the `custom_GRU()` function prior to the sigmoid and tanh operations. The last helper function is `add_dropout_and_layers()` which will add dropout to our recurrent outputs and will allow us to create multi-layered recurrent architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Operation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     10,
     15,
     27,
     32,
     36,
     62,
     107,
     127,
     165
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Operation functions for model.py\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers import (\n",
    "    utils,\n",
    ")\n",
    "\n",
    "from tensorflow.python.framework import (\n",
    "    ops,\n",
    "    tensor_shape,\n",
    ")\n",
    "\n",
    "from tensorflow.python.ops import (\n",
    "        gen_array_ops,\n",
    "        array_ops,\n",
    "        clip_ops,\n",
    "        embedding_ops,\n",
    "        init_ops,\n",
    "        math_ops,\n",
    "        nn_ops,\n",
    "        partitioned_variables,\n",
    "        variable_scope as vs,\n",
    ")\n",
    "\n",
    "from tensorflow.python.ops.math_ops import (\n",
    "        sigmoid,\n",
    "        tanh,\n",
    ")\n",
    "\n",
    "from tensorflow.python.util import (\n",
    "    nest,\n",
    ")\n",
    "\n",
    "def _xavier_weight_init(nonlinearity='tanh'):\n",
    "    \"\"\"\n",
    "    Xavier weights initialization.\n",
    "    \"\"\"\n",
    "    def _xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"\n",
    "        Tanh and sigmoid initialization.\n",
    "        \"\"\"\n",
    "        eps = 1.0 / np.sqrt(np.sum(shape))\n",
    "        return tf.random_uniform(shape, minval=-eps, maxval=eps)\n",
    "\n",
    "    def _relu_xavier_initializer(shape, **kwargs):\n",
    "        \"\"\"\n",
    "        ReLU initialization.\n",
    "        \"\"\"\n",
    "        eps = np.sqrt(2.0) / np.sqrt(np.sum(shape))\n",
    "        return tf.random_uniform(shape, minval=-eps, maxval=eps)\n",
    "\n",
    "    if nonlinearity in ('tanh', 'sigmoid'):\n",
    "        return _xavier_initializer\n",
    "    elif nonlinearity in ('relu'):\n",
    "        return _relu_xavier_initializer\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Please choose a valid nonlinearity: tanh|sigmoid|relu\")\n",
    "\n",
    "def _linear(args, output_size, bias, bias_start=0.0,\n",
    "    nonlinearity='relu', scope=None, name=None):\n",
    "    \"\"\"\n",
    "    Sending inputs through a two layer MLP.\n",
    "    Args:\n",
    "        args: list of inputs of shape (N, H)\n",
    "        output_size: second dimension of W\n",
    "        bias: boolean, whether or not to add bias\n",
    "        bias_start: initial bias value\n",
    "        nonlinearity: nonlinear transformation to use (tanh|sigmoid|relu)\n",
    "        scope: (optional) Variable scope to create parameters in.\n",
    "        name: (optional) variable name.\n",
    "    Returns:\n",
    "        Tensor with shape (N, output_size)\n",
    "    \"\"\"\n",
    "    _input = tf.concat(\n",
    "        values=args,\n",
    "        axis=1,)\n",
    "    shape = _input.get_shape()\n",
    "    # Computation\n",
    "    scope = vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        w_name = \"W_1_\"\n",
    "        if name is not None:\n",
    "            w_name += name\n",
    "        W_1 = vs.get_variable(\n",
    "            name=w_name,\n",
    "            shape=[shape[1], output_size],\n",
    "            initializer=_xavier_weight_init(\n",
    "                nonlinearity=nonlinearity),\n",
    "            )\n",
    "        result_1 = tf.matmul(_input, W_1)\n",
    "        if bias:\n",
    "            b_name = \"b_1_\"\n",
    "            if name is not None:\n",
    "                b_name += name\n",
    "            b_1 = vs.get_variable(\n",
    "                name=b_name,\n",
    "                shape=(output_size,),\n",
    "                initializer=init_ops.constant_initializer(\n",
    "                    bias_start, dtype=tf.float32),\n",
    "                )\n",
    "            result_1 = tf.add(result_1, b_1)\n",
    "    return result_1\n",
    "\n",
    "def ln(inputs, epsilon=1e-5, scope=None):\n",
    "\n",
    "    \"\"\" Computer layer norm given an input tensor. We get in an input of shape\n",
    "    [N X D] and with LN we compute the mean and var for each individual\n",
    "    training point across all it's hidden dimensions rather than across\n",
    "    the training batch as we do in BN. This gives us a mean and var of shape\n",
    "    [N X 1].\n",
    "    \"\"\"\n",
    "    mean, var = tf.nn.moments(inputs, [1], keep_dims=True)\n",
    "    with tf.variable_scope(scope + 'LN'):\n",
    "            scale = tf.get_variable('alpha',\n",
    "                shape=[inputs.get_shape()[1]],\n",
    "                initializer=tf.constant_initializer(1))\n",
    "            shift = tf.get_variable('beta',\n",
    "                shape=[inputs.get_shape()[1]],\n",
    "                initializer=tf.constant_initializer(0))\n",
    "    LN = scale * (inputs - mean) / tf.sqrt(var + epsilon) + shift\n",
    "\n",
    "    return LN\n",
    "\n",
    "class custom_GRUCell(tf.contrib.rnn.RNNCell):\n",
    "        \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n",
    "\n",
    "        def __init__(self, num_units, input_size=None, activation=tanh):\n",
    "            if input_size is not None:\n",
    "                logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "            self._num_units = num_units\n",
    "            self._activation = activation\n",
    "\n",
    "        @property\n",
    "        def state_size(self):\n",
    "            return self._num_units\n",
    "\n",
    "        @property\n",
    "        def output_size(self):\n",
    "            return self._num_units\n",
    "\n",
    "        def __call__(self, inputs, state, scope=None):\n",
    "            \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "            with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n",
    "                with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n",
    "                    # We start with bias of 1.0 to not reset and not update.\n",
    "                    r, u = array_ops.split(\n",
    "                            _linear([inputs, state], 2 * self._num_units, True, 1.0), 2, 1,\n",
    "                    )\n",
    "\n",
    "                    # Apply Layer Normalization to the two gates\n",
    "                    r = ln(r, scope = 'r/')\n",
    "                    u = ln(r, scope = 'u/')\n",
    "\n",
    "                    r, u = sigmoid(r), sigmoid(u)\n",
    "                with vs.variable_scope(\"Candidate\"):\n",
    "                    c = self._activation(\n",
    "                        _linear([inputs, r * state],\n",
    "                            self._num_units, True))\n",
    "                new_h = u * state + (1 - u) * c\n",
    "            return new_h, new_h\n",
    "\n",
    "def add_dropout_and_layers(single_cell, keep_prob, num_layers):\n",
    "    \"\"\"\n",
    "    Add dropout and create stacked layers using a single_cell.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropout\n",
    "    stacked_cell = tf.contrib.rnn.DropoutWrapper(single_cell,\n",
    "        output_keep_prob=keep_prob)\n",
    "\n",
    "    # Each state as one cell\n",
    "    if num_layers > 1:\n",
    "        stacked_cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [single_cell] * num_layers)\n",
    "\n",
    "    return stacked_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Let's briefly describe the model pipelines and see how our inputs undergo representation changes. First we will initialize our placeholders which will hold the reviews, lens, sentiment, embeddings etc. Then we will build the encoder which will take our input review and first embed using the GloVe embeddings. We will then feed the embedded tokens into a GRU in order to encode the input. We will use the output from each timestep in the GRU as our inputs to the attentional layer. Notice that we could have completely removed the attentional interface, and just used the last relevant hidden state from the encoder GRU in order to receive our predicted sentiment, but -- adding this attention layer allows us to see how the model processes the input review.\n",
    "\n",
    "In the attentional layer, we apply a nonlinearity followed by another one, in order to reduce our representation to one dimension. Now, we can normalize to compute our attention scores. These scores are then broadcasted and multiplied with the original inputs to receive our summarized vector. We use this vector to receive our predicted sentiment via normalization in the decoder. Notice that we do not use a previous state ($s_{i-1}$) since the task involves creating just one context and extracting the sentiment from that.\n",
    "\n",
    "We then define our loss as the cross entropy between the predicted and the ground truth sentiment. We use a bit of decay for our learning rate with an absolute minimum and use the ADAM optimizer [9]. With all of these components, we have built our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     10,
     16,
     38,
     57,
     75,
     108,
     175,
     254,
     275,
     287,
     312
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple GRU Encoder/Decoder Model w/ Attentional Interface\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    \"\"\"\n",
    "    Tensorflow graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, FLAGS, vocab_size):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.FLAGS = FLAGS\n",
    "        self._vsize = vocab_size\n",
    "\n",
    "    def train(self, sess, batch_reviews, batch_labels,\n",
    "        batch_review_lens, embeddings, keep_prob):\n",
    "        \"\"\"\n",
    "        Train the model using a batch and predicted guesses.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            self._train_op,\n",
    "            self._logits,\n",
    "            self._loss,\n",
    "            self._accuracy,\n",
    "            self._lr,\n",
    "            self._Z,\n",
    "                   ]\n",
    "        inputs = {\n",
    "            self._reviews: batch_reviews,\n",
    "            self._labels: batch_labels,\n",
    "            self._review_lens: batch_review_lens,\n",
    "            self._embeddings: embeddings,\n",
    "            self._keep_prob: keep_prob,\n",
    "                  }\n",
    "        return sess.run(outputs, inputs)\n",
    "\n",
    "    def eval(self, sess, batch_reviews, batch_labels,\n",
    "        batch_review_lens, embeddings, keep_prob=1.0):\n",
    "        \"\"\"\n",
    "        Evaluation of validation set.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            self._logits,\n",
    "            self._loss,\n",
    "            self._accuracy,\n",
    "        ]\n",
    "        inputs = {\n",
    "            self._reviews: batch_reviews,\n",
    "            self._labels: batch_labels,\n",
    "            self._review_lens: batch_review_lens,\n",
    "            self._embeddings: embeddings,\n",
    "            self._keep_prob: keep_prob,\n",
    "            }\n",
    "        return sess.run(outputs, inputs)\n",
    "\n",
    "    def infer(self, sess, batch_reviews,\n",
    "        batch_review_lens, embeddings, keep_prob=1.0):\n",
    "        \"\"\"\n",
    "        Inference with a sample sentence.\n",
    "        \"\"\"\n",
    "        outputs = [\n",
    "            self._logits,\n",
    "            self._probabilities,\n",
    "            self._Z,\n",
    "        ]\n",
    "        inputs = {\n",
    "            self._reviews: batch_reviews,\n",
    "            self._review_lens: batch_review_lens,\n",
    "            self._embeddings: embeddings,\n",
    "            self._keep_prob: keep_prob,\n",
    "            }\n",
    "        return sess.run(outputs, inputs)\n",
    "\n",
    "    def _add_placeholders(self):\n",
    "        \"\"\"\n",
    "        Input that will be fed into our DCN graph.\n",
    "        \"\"\"\n",
    "        print (\"==> Adding placeholders:\")\n",
    "\n",
    "        FLAGS = self.FLAGS\n",
    "        self._reviews = tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None, FLAGS.max_input_length],\n",
    "            name=\"reviews\")\n",
    "        self._review_lens = tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None, ],\n",
    "            name=\"review_lens\")\n",
    "        self._labels = tf.placeholder(\n",
    "            dtype=tf.int32,\n",
    "            shape=[None,],\n",
    "            name=\"labels\")\n",
    "        self._embeddings = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=(FLAGS.vocab_size, FLAGS.emb_size),\n",
    "            name='glove_embeddings')\n",
    "        self._keep_prob = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=(),\n",
    "            name=\"keep_prob\")\n",
    "\n",
    "        print (\"\\t self._reviews:\", self._reviews.get_shape())\n",
    "        print (\"\\t self._labels:\", self._labels.get_shape())\n",
    "        print (\"\\t self._embeddings:\", self._embeddings.get_shape())\n",
    "        print (\"\\t self._keep_prob:\", self._keep_prob.get_shape())\n",
    "\n",
    "    def _build_encoder(self):\n",
    "        \"\"\"\n",
    "        Constructing the encoder.\n",
    "        \"\"\"\n",
    "        print (\"==> Building the encoder:\")\n",
    "\n",
    "        FLAGS = self.FLAGS\n",
    "        batch_size = FLAGS.batch_size\n",
    "        hidden_size = FLAGS.hidden_size\n",
    "        max_input_length = FLAGS.max_input_length\n",
    "\n",
    "        with tf.variable_scope('embedding'):\n",
    "            print (\"\\t embedding:\")\n",
    "\n",
    "            if FLAGS.embedding == 'random':\n",
    "                # Random embedding weights\n",
    "                embedding = tf.get_variable(\n",
    "                    name='embedding',\n",
    "                    shape=[self._vsize, FLAGS.emb_size],\n",
    "                    dtype=tf.float32,\n",
    "                    initializer=tf.truncated_normal_initializer(stddev=1e-4),\n",
    "                    trainable=FLAGS.train_embedding)\n",
    "            elif FLAGS.embedding == 'glove':\n",
    "                # GloVe embedding weights\n",
    "                embedding = self._embeddings\n",
    "\n",
    "            # Check embedding dim\n",
    "            if embedding.get_shape()[1] != FLAGS.emb_size:\n",
    "                raise Exception(\n",
    "                    \"Embedding's dimension does not match specified emb_size.\")\n",
    "\n",
    "            # Embedding the review\n",
    "            fn = lambda x: tf.nn.embedding_lookup(embedding, x)\n",
    "            c_embedding = tf.map_fn(\n",
    "                lambda x: fn(x), self._reviews, dtype=tf.float32)\n",
    "\n",
    "            print (\"\\t\\t embedding:\", embedding.get_shape())\n",
    "            print (\"\\t\\t reviews_embedded:\", c_embedding.get_shape())\n",
    "\n",
    "        with tf.variable_scope('c_encoding'):\n",
    "            print (\"\\t c_encoding:\")\n",
    "\n",
    "            # GRU cells\n",
    "            cell = add_dropout_and_layers(\n",
    "                single_cell=custom_GRUCell(hidden_size),\n",
    "                keep_prob=self._keep_prob,\n",
    "                num_layers=FLAGS.num_layers,\n",
    "                )\n",
    "\n",
    "            # Dynamic-GRU\n",
    "            # return (outputs, last_output_states (relevant))\n",
    "            all_outputs, h = tf.nn.dynamic_rnn(\n",
    "                cell=cell,\n",
    "                inputs=c_embedding,\n",
    "                #sequence_length=self._review_lens,\n",
    "                dtype=tf.float32,\n",
    "                time_major=False,\n",
    "                )\n",
    "\n",
    "            self._all_outputs = all_outputs\n",
    "            self._h = h\n",
    "\n",
    "            self._z = all_outputs\n",
    "\n",
    "            print (\"\\t\\t self._all_outputs\", self._all_outputs.get_shape())\n",
    "            print (\"\\t\\t self._h\", self._h.get_shape())\n",
    "\n",
    "    def _build_attentional_interface(self):\n",
    "        \"\"\"\n",
    "        Adding an attentional interface\n",
    "        for model interpretability.\n",
    "        \"\"\"\n",
    "        print (\"==> Building the attentional interface:\")\n",
    "\n",
    "        FLAGS = self.FLAGS\n",
    "        batch_size = FLAGS.batch_size\n",
    "        hidden_size = FLAGS.hidden_size\n",
    "        max_input_length = FLAGS.max_input_length\n",
    "        loop_until = tf.to_int32(np.array(range(batch_size)))\n",
    "\n",
    "        with tf.variable_scope('attention') as attn_scope:\n",
    "            print (\"\\t attention:\")\n",
    "\n",
    "            # Time-major self._all_outputs (N, M, H) --> (M, N, H)\n",
    "            all_outputs_time_major = tf.transpose(self._all_outputs,\n",
    "                perm=[1,0,2])\n",
    "\n",
    "            # Apply tanh nonlinearity\n",
    "            fn = lambda _input: tf.nn.tanh(_linear(\n",
    "                    args=_input,\n",
    "                    output_size=hidden_size,\n",
    "                    bias=True,\n",
    "                    bias_start=0.0,\n",
    "                    nonlinearity='tanh',\n",
    "                    scope=attn_scope,\n",
    "                    name='attn_nonlinearity',\n",
    "                    ))\n",
    "            z = tf.map_fn(\n",
    "                lambda x: fn(x), all_outputs_time_major, dtype=tf.float32)\n",
    "\n",
    "            # Apply softmax weights\n",
    "            fn = lambda _input: tf.nn.tanh(_linear(\n",
    "                    args=_input,\n",
    "                    output_size=1,\n",
    "                    bias=True,\n",
    "                    bias_start=0.0,\n",
    "                    nonlinearity='tanh',\n",
    "                    scope=attn_scope,\n",
    "                    name='attn_softmax',\n",
    "                    ))\n",
    "            z = tf.map_fn(\n",
    "                lambda x: fn(x), z, dtype=tf.float32)\n",
    "\n",
    "            # Squeeze and convert to batch major\n",
    "            z = tf.transpose(\n",
    "                    tf.squeeze(\n",
    "                        input=z,\n",
    "                        axis=2,\n",
    "                        ),\n",
    "                    perm=[1,0])\n",
    "\n",
    "            # Normalize\n",
    "            self._Z = tf.nn.softmax(\n",
    "                logits=z,\n",
    "                )\n",
    "\n",
    "            # Create context vector (via soft attention.)\n",
    "            fn = lambda sample_num: \\\n",
    "                tf.reduce_sum(\n",
    "                    tf.multiply(\n",
    "                        self._all_outputs[sample_num][:self._review_lens[sample_num]],\n",
    "\n",
    "                        # (500,) --> (500, 1) --> (500, 200)\n",
    "                        tf.tile(\n",
    "                            input=tf.expand_dims(\n",
    "                                self._Z[sample_num][:self._review_lens[sample_num]], 1),\n",
    "                            multiples=(1, hidden_size),\n",
    "                        )),\n",
    "                    axis=0)\n",
    "\n",
    "            self._c = tf.map_fn(\n",
    "                lambda sample_num: fn(sample_num), loop_until, dtype=tf.float32)\n",
    "\n",
    "            print (\"\\t\\t self._Z\", self._Z.get_shape())\n",
    "            print (\"\\t\\t self._c\", self._c.get_shape())\n",
    "\n",
    "    def _build_decoder(self):\n",
    "        \"\"\"\n",
    "        Applying a softmax on output of encoder.\n",
    "        \"\"\"\n",
    "        print (\"==> Building the decoder:\")\n",
    "        with tf.variable_scope('softmax'):\n",
    "            print (\"\\t Softmax:\")\n",
    "            self._logits = _linear(\n",
    "                args=self._c, # self._c (with attn) or self._h (no attn)\n",
    "                output_size=self.FLAGS.num_classes,\n",
    "                bias=True,\n",
    "                bias_start=0.0,\n",
    "                nonlinearity='relu',\n",
    "                name='softmax_op',\n",
    "                )\n",
    "            self._probabilities = tf.nn.softmax(\n",
    "                logits=self._logits,\n",
    "                )\n",
    "            print (\"\\t\\t self._logits\", self._logits.get_shape())\n",
    "            print (\"\\t\\t self._probabilities\", self._probabilities.get_shape())\n",
    "\n",
    "    def _add_loss(self):\n",
    "        \"\"\"\n",
    "        Determine the loss.\n",
    "        \"\"\"\n",
    "        print (\"==> Establishing the loss function.\")\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self._labels, logits=self._logits))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self._logits, 1),\n",
    "            tf.cast(self._labels, tf.int64)), tf.float32))\n",
    "        return self.loss, self.accuracy\n",
    "\n",
    "    def _add_train_op(self):\n",
    "        \"\"\"\n",
    "        Add the training optimizer.\n",
    "        \"\"\"\n",
    "        print (\"==> Creating the training optimizer.\")\n",
    "\n",
    "        # Decay learning rate\n",
    "        self._lr = tf.maximum(\n",
    "            self.FLAGS.min_lr,\n",
    "            tf.train.exponential_decay(\n",
    "                learning_rate=self.FLAGS.lr,\n",
    "                global_step=self.global_step,\n",
    "                decay_steps=100000,\n",
    "                decay_rate=self.FLAGS.decay_rate,\n",
    "                staircase=False,\n",
    "                ))\n",
    "\n",
    "        # Training releaved no clipping needed\n",
    "\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self._lr).minimize(self.loss,\n",
    "            global_step=self.global_step)\n",
    "        return self.optimizer\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"\n",
    "        Contrust each component of the TF graph.\n",
    "        \"\"\"\n",
    "        self._add_placeholders()\n",
    "        self._build_encoder()\n",
    "        self._build_attentional_interface()\n",
    "        self._build_decoder()\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False) # won't step\n",
    "        if self.FLAGS.mode == 'train':\n",
    "            self._loss, self._accuracy = self._add_loss()\n",
    "            self._train_op = self._add_train_op()\n",
    "\n",
    "        # Components for model saving\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        print (\"==> Review Classifier built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='8'></a>\n",
    "### **VIII. Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def generate_epoch(data_path, num_epochs, batch_size):\n",
    "    \"\"\"\n",
    "    Generate num_epoch epochs.\n",
    "    Args:\n",
    "        data_path: path for train.p|valid.p\n",
    "        num_epochs: number of epochs to run for\n",
    "        batch_size: samples per each batch\n",
    "    \"\"\"\n",
    "    with open(data_path, 'rb') as f:\n",
    "        entries = pickle.load(f)\n",
    "\n",
    "    processed_contexts, processed_answers = [], []\n",
    "    context_lens = []\n",
    "\n",
    "    for entry in entries:\n",
    "        processed_contexts.append(entry[0])\n",
    "        context_lens.append(entry[1])\n",
    "        processed_answers.append(entry[2])\n",
    "\n",
    "    features = [processed_contexts, processed_answers]\n",
    "    seq_lens = [context_lens,]\n",
    "\n",
    "    for epoch_num in range(num_epochs):\n",
    "        yield generate_batch(features, seq_lens, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def generate_batch(features, seq_lens, batch_size):\n",
    "    \"\"\"\n",
    "    Generate batches of size <batch_size>.\n",
    "    Args:\n",
    "        features: processed contexts, questions and answers.\n",
    "        seq_lens: context and question actual (pre-padding) seq-lens.\n",
    "        batch_size: samples per each batch\n",
    "    \"\"\"\n",
    "    data_size = len(features[0])\n",
    "    num_batches = data_size//batch_size\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num+1)*batch_size, data_size)\n",
    "\n",
    "        batch_features = []\n",
    "        for feature in features:\n",
    "            batch_features.append(feature[start_index:end_index])\n",
    "        batch_lens = []\n",
    "        for seq_len in seq_lens:\n",
    "            batch_lens.append(seq_len[start_index:end_index])\n",
    "\n",
    "        yield batch_features, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"  \n",
    "        self.data_dir=\"data/processed_reviews\"           # location of reviews data\n",
    "        self.ckpt_dir=\"data/processed_reviews/ckpt\"      # location of model checkpoints\n",
    "        self.mode=\"train\"                                # train|infer\n",
    "        self.model=\"new\"                                 # old|new\n",
    "        self.lr=1e-4                                     # learning rate\n",
    "        self.num_epochs=1                                # num of epochs \n",
    "        self.batch_size=256                              # batch size\n",
    "        self.hidden_size=200                             # num hidden units for RNN\n",
    "        self.embedding=\"glove\"                           # random|glove\n",
    "        self.emb_size=200                                # num hidden units for embeddings\n",
    "        self.max_grad_norm=5                             # max gradient norm\n",
    "        self.keep_prob=0.9                               # Keep prob for dropout layers\n",
    "        self.num_layers=1                                # number of layers for recurrsion\n",
    "        self.max_input_length=300                        # max number of words per review\n",
    "        self.min_lr=1e-6                                 # minimum learning rate\n",
    "        self.decay_rate=0.96                             # Decay rate for lr per global step (train batch)\n",
    "        self.save_every=10                               # Save the model every <save_every> epochs\n",
    "        self.model_name=\"imdb_model\"                     # Name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def create_model(sess, FLAGS, vocab_size):\n",
    "    \"\"\"\n",
    "    Creates a new model or loads old one.\n",
    "    \"\"\"\n",
    "    imdb_model = Model(FLAGS, vocab_size)\n",
    "    imdb_model._build_graph()\n",
    "\n",
    "    if FLAGS.model == 'new':\n",
    "        print ('==> Created a new model.')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    elif FLAGS.model == 'old':\n",
    "        ckpt = tf.train.get_checkpoint_state(\n",
    "            os.path.join(basedir, FLAGS.ckpt_dir))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            print(\"==> Restoring old model parameters from %s\" %\n",
    "                ckpt.model_checkpoint_path)\n",
    "            imdb_model.saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print (\"==> No old model to load from so initializing a new one.\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    return imdb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def train(FLAGS):\n",
    "    \"\"\"\n",
    "    Train a previous or new model.\n",
    "    \"\"\"\n",
    "    # Data paths\n",
    "    vocab_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/vocab.txt')\n",
    "    train_data_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/train.p')\n",
    "    validation_data_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/validation.p')\n",
    "    vocab = Vocab(vocab_path)\n",
    "    FLAGS.num_classes = 2\n",
    "\n",
    "    # Load embeddings (if using GloVe)\n",
    "    if FLAGS.embedding == 'glove':\n",
    "        with open(os.path.join(\n",
    "            basedir, 'data/processed_reviews/embeddings.p'), 'rb') as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        FLAGS.vocab_size = len(embeddings)\n",
    "\n",
    "    # Start tensorflow session\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Create|reload model\n",
    "        imdb_model = create_model(sess, FLAGS, len(vocab))\n",
    "\n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            \"train_loss\": [],\n",
    "            \"valid_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"valid_acc\": [],\n",
    "        }\n",
    "\n",
    "        # Store attention score history for few samples\n",
    "        attn_history = {\n",
    "            \"sample_0\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_1\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_2\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_3\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "            \"sample_4\":\n",
    "            {\"review\": None, \"label\": None, \"review_len\": None, \"attn_scores\": []},\n",
    "        }\n",
    "\n",
    "        # Start training\n",
    "        for train_epoch_num, train_epoch in \\\n",
    "            enumerate(generate_epoch(\n",
    "                train_data_path, FLAGS.num_epochs, FLAGS.batch_size)):\n",
    "\n",
    "            print (\"==> EPOCH:\", train_epoch_num)\n",
    "\n",
    "            for train_batch_num, (batch_features, batch_seq_lens) in \\\n",
    "                enumerate(train_epoch):\n",
    "\n",
    "                batch_reviews, batch_labels = batch_features\n",
    "                batch_review_lens, = batch_seq_lens\n",
    "\n",
    "                # Display shapes once\n",
    "                if (train_epoch_num == 0 and train_batch_num == 0):\n",
    "                    print (\"Reviews: \", np.shape(batch_reviews))\n",
    "                    print (\"Labels: \", np.shape(batch_labels))\n",
    "                    print (\"Review lens: \", np.shape(batch_review_lens))\n",
    "\n",
    "                _, train_logits, train_loss, train_acc, lr, attn_scores = \\\n",
    "                    imdb_model.train(\n",
    "                        sess=sess,\n",
    "                        batch_reviews=batch_reviews,\n",
    "                        batch_labels=batch_labels,\n",
    "                        batch_review_lens=batch_review_lens,\n",
    "                        embeddings=embeddings,\n",
    "                        keep_prob=FLAGS.keep_prob,\n",
    "                        )\n",
    "\n",
    "            for valid_epoch_num, valid_epoch in \\\n",
    "                enumerate(generate_epoch(\n",
    "                    data_path=validation_data_path,\n",
    "                    num_epochs=1,\n",
    "                    batch_size=FLAGS.batch_size,\n",
    "                    )):\n",
    "\n",
    "                for valid_batch_num, (valid_batch_features, valid_batch_seq_lens) in \\\n",
    "                    enumerate(valid_epoch):\n",
    "\n",
    "                    valid_batch_reviews, valid_batch_labels = valid_batch_features\n",
    "                    valid_batch_review_lens, = valid_batch_seq_lens\n",
    "\n",
    "                    valid_logits, valid_loss, valid_acc = imdb_model.eval(\n",
    "                        sess=sess,\n",
    "                        batch_reviews=valid_batch_reviews,\n",
    "                        batch_labels=valid_batch_labels,\n",
    "                        batch_review_lens=valid_batch_review_lens,\n",
    "                        embeddings=embeddings,\n",
    "                        keep_prob=1.0, # no dropout for val|test\n",
    "                        )\n",
    "\n",
    "            print (\"[EPOCH]: %i, [LR]: %.6e, [TRAIN ACC]: %.3f, [VALID ACC]: %.3f \" \\\n",
    "                   \"[TRAIN LOSS]: %.6f, [VALID LOSS]: %.6f\" % (\n",
    "                train_epoch_num, lr, train_acc, valid_acc, train_loss, valid_loss))\n",
    "\n",
    "            # Store the metrics\n",
    "            metrics[\"train_loss\"].append(train_loss)\n",
    "            metrics[\"valid_loss\"].append(valid_loss)\n",
    "            metrics[\"train_acc\"].append(train_acc)\n",
    "            metrics[\"valid_acc\"].append(valid_acc)\n",
    "\n",
    "            # Store attn history\n",
    "            for i in range(5):\n",
    "                sample = \"sample_%i\"%i\n",
    "                attn_history[sample][\"review\"] = batch_reviews[i]\n",
    "                attn_history[sample][\"label\"] = batch_labels[i]\n",
    "                attn_history[sample][\"review_len\"] = batch_review_lens[i]\n",
    "                attn_history[sample][\"attn_scores\"].append(attn_scores[i])\n",
    "\n",
    "            # Save the model (maybe)\n",
    "            if ((train_epoch_num == (FLAGS.num_epochs-1)) or\n",
    "            ((train_epoch_num%FLAGS.save_every == 0) and (train_epoch_num>0))):\n",
    "\n",
    "                # Make parents ckpt dir if it does not exist\n",
    "                if not os.path.isdir(os.path.join(basedir, FLAGS.data_dir, 'ckpt')):\n",
    "                    os.makedirs(os.path.join(basedir, FLAGS.data_dir, 'ckpt'))\n",
    "\n",
    "                # Make child ckpt dir for this specific model\n",
    "                if not os.path.isdir(os.path.join(basedir, FLAGS.ckpt_dir)):\n",
    "                    os.makedirs(os.path.join(basedir, FLAGS.ckpt_dir))\n",
    "\n",
    "                checkpoint_path = \\\n",
    "                    os.path.join(\n",
    "                        basedir, FLAGS.ckpt_dir, \"%s.ckpt\" % FLAGS.model_name)\n",
    "\n",
    "                print (\"==> Saving the model.\")\n",
    "                imdb_model.saver.save(sess, checkpoint_path,\n",
    "                                 global_step=imdb_model.global_step)\n",
    "\n",
    "    # Save the metrics\n",
    "    metrics_file = os.path.join(basedir, FLAGS.ckpt_dir, 'metrics.p')\n",
    "    with open(metrics_file, 'wb') as f:\n",
    "        pickle.dump(metrics, f)\n",
    "\n",
    "    # Save the attention scores\n",
    "    attn_history_file = os.path.join(basedir, FLAGS.ckpt_dir, 'attn_history.p')\n",
    "    with open(attn_history_file, 'wb') as f:\n",
    "        pickle.dump(attn_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "FLAGS = parameters()\n",
    "train(FLAGS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='9'></a>\n",
    "### **IX. Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.ckpt_dir=\"data/processed_reviews/ckpt\"      # location of model checkpoints\n",
    "        self.model_name=\"imdb_model\"                     # Name of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def plot_metrics(FLAGS):\n",
    "    \"\"\"\n",
    "    Plot the loss and accuracy for train|test.\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Load metrics from file\n",
    "    metrics_file = os.path.join(basedir, FLAGS.ckpt_dir, 'metrics.p')\n",
    "    with open(metrics_file, 'rb') as f:\n",
    "        metrics = pickle.load(f)\n",
    "        \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot results\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(metrics[\"train_acc\"], label='train accuracy')\n",
    "    ax1.plot(metrics[\"valid_acc\"], label='valid accuracy')\n",
    "    ax1.legend(loc=4)\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('train|valid accuracy')\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(metrics[\"train_loss\"], label='train loss')\n",
    "    ax2.plot(metrics[\"valid_loss\"], label='valid loss')\n",
    "    ax2.legend(loc=3)\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('train|valid loss')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "# Add model name to ckpt dir\n",
    "FLAGS.ckpt_dir = FLAGS.ckpt_dir + '/%s'%(FLAGS.model_name)\n",
    "plot_metrics(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can see a bit of overfitting after ~epoch 7. If you want to achieve the best performance, use all 25,000 training/test samples and include a lot more stringent regularization along with gradient clipping a more rigorous decay. But since just wanted to see some interpretable attention scores, this performance was satifactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='10'></a>\n",
    "### **X. Attention for a Sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "class parameters():\n",
    "    \"\"\"\n",
    "    Arguments for data processing.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.data_dir=\"data/processed_reviews\"           # location of reviews data\n",
    "        self.ckpt_dir=\"data/processed_reviews/ckpt\"      # location of model checkpoints\n",
    "        self.model_name=\"imdb_model\"                     # Name of the model\n",
    "        self.sample_num=2                                # Sample num to view attn plot. [0-4]\n",
    "        self.num_rows=5                                  # Number of rows to show in attn visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def plot_attn(input_sentence, attentions, num_rows, save_loc=None):\n",
    "    \"\"\"\n",
    "    Plot the attention scores.\n",
    "    Args:\n",
    "        input_sentence: input sentence (tokens) without <pad>\n",
    "        attentions: attention scores for each token in input_sentence\n",
    "        num_rows: how many rows you want the figure to have (we will add 1)\n",
    "        save_loc: fig will be saved to this location\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine how many words per row\n",
    "    words_per_row = (len(input_sentence.split(' '))//num_rows)\n",
    "\n",
    "    # Use one extra row in case of remained for quotient above\n",
    "    fig, axes = plt.subplots(nrows=num_rows+1, ncols=1, figsize=(20, 10))\n",
    "    for row_num, ax in enumerate(axes.flat):\n",
    "\n",
    "        # Isolate pertinent part of sentence and attention scores\n",
    "        start_index = row_num*words_per_row\n",
    "        end_index = (row_num*words_per_row)+words_per_row\n",
    "        _input_sentence = \\\n",
    "            input_sentence.split(' ')[start_index:end_index]\n",
    "        _attentions = np.reshape(\n",
    "            attentions[0, start_index:end_index],\n",
    "            (1, len(attentions[0, start_index:end_index]))\n",
    "            )\n",
    "\n",
    "        # Plot attn scores (constrained to [0.9, 1] for emphasis)\n",
    "        im = ax.imshow(_attentions, cmap='Blues', vmin=0.9, vmax=1)\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_xticklabels(\n",
    "            [''] + _input_sentence,\n",
    "            rotation=90,\n",
    "            minor=False,\n",
    "            )\n",
    "        ax.set_yticklabels([''])\n",
    "\n",
    "        # Set x tick to top\n",
    "        ax.xaxis.set_ticks_position('top')\n",
    "        ax.tick_params(axis='x', colors='black')\n",
    "\n",
    "        # Show corresponding words at the ticks\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # Add color bar\n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n",
    "\n",
    "    # display color bar\n",
    "    cb = fig.colorbar(im, cax=cbar)\n",
    "    cb.set_ticks([]) # clean color bar\n",
    "\n",
    "    if save_loc is None:\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Save the plot\n",
    "        fig.savefig(save_loc, dpi=fig.dpi, bbox_inches='tight') # dpi=fig.dpi for high res. save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def get_attn_inputs(FLAGS, review, review_len, raw_attn_scores):\n",
    "    \"\"\"\n",
    "    Return the inputs needed to\n",
    "    plot the attn scores. These include\n",
    "    input_sentence and attn_scores.\n",
    "    Args:\n",
    "        FLAGS: parameters\n",
    "        review: list of ids\n",
    "        review_len: len of the relevant review\n",
    "    Return:\n",
    "        input_sentence: inputs as tokens (words) on len <review_len>\n",
    "        plot_attn_scoes: (1, review_len) shaped scores\n",
    "    \"\"\"\n",
    "\n",
    "    review_len = 300\n",
    "\n",
    "    # Data paths\n",
    "    vocab_path = os.path.join(\n",
    "        basedir, 'data/processed_reviews/vocab.txt')\n",
    "    vocab = Vocab(vocab_path)\n",
    "\n",
    "    review = review[:review_len]\n",
    "    attn_scores = raw_attn_scores[:review_len]\n",
    "\n",
    "    # Process input_sentence\n",
    "    input_sentence = ' '.join([item for item in ids_to_tokens(review, vocab)])\n",
    "\n",
    "    # Process attn scores (normalize scores between [0,1])\n",
    "    min_attn_score = min(attn_scores)\n",
    "    max_attn_score = max(attn_scores)\n",
    "    normalized_attn_scores = ((attn_scores - min_attn_score) / \\\n",
    "        (max_attn_score - min_attn_score))\n",
    "\n",
    "    # Reshape attn scores for plotting\n",
    "    plot_attn_scores = np.zeros((1, review_len))\n",
    "    for i, score in enumerate(normalized_attn_scores):\n",
    "        plot_attn_scores[0, i] = score\n",
    "\n",
    "    return input_sentence, plot_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def process_sample_attn(FLAGS):\n",
    "    \"\"\"\n",
    "    Use plot_attn from utils.py to visualize\n",
    "    the attention scores for a particular\n",
    "    sample FLAGS.sample_num.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the attn history\n",
    "    attn_history_path = os.path.join(\n",
    "        basedir, FLAGS.ckpt_dir, 'attn_history.p')\n",
    "    with open(attn_history_path, 'rb') as f:\n",
    "        attn_history = pickle.load(f)\n",
    "\n",
    "    # Process the history to get the right sample\n",
    "    sample = \"sample_%i\" % (FLAGS.sample_num)\n",
    "    review_len = attn_history[sample][\"review_len\"]\n",
    "    review = attn_history[sample][\"review\"]\n",
    "    label = attn_history[sample][\"label\"]\n",
    "    attn_scores = attn_history[sample][\"attn_scores\"][-1]\n",
    "\n",
    "    input_sentence, plot_attn_scores = get_attn_inputs(\n",
    "        FLAGS=FLAGS,\n",
    "        review=review,\n",
    "        review_len=review_len,\n",
    "        raw_attn_scores=attn_scores,\n",
    "        )\n",
    "\n",
    "    # Plot and save fig\n",
    "    fig_name = \"sample_%i\" % (FLAGS.sample_num)\n",
    "    save_loc = os.path.join(basedir, FLAGS.ckpt_dir, fig_name)\n",
    "    plot_attn(\n",
    "        input_sentence=input_sentence,\n",
    "        attentions=plot_attn_scores,\n",
    "        num_rows=FLAGS.num_rows,\n",
    "        save_loc=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "FLAGS = parameters()\n",
    "# Add model name to ckpt dir\n",
    "FLAGS.ckpt_dir = FLAGS.ckpt_dir + '/%s'%(FLAGS.model_name)\n",
    "process_sample_attn(FLAGS)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
